# 1) Overview

A Streamlit web app where users upload documents (PDF/DOCX/TXT/MD), the system indexes them into a local RAG pipeline, and users chat in German or English with citations back to the sources. Runs on a **single server** (CPU or 1Ã—GPU), minimal services, low ops overhead.

**Guiding principles**

* â€œSimple, but not simplerâ€
* Prefer wellâ€‘tested, lightweight components
* Localâ€‘first (no external SaaS required)

---

# 2) Toolstack (final choices)

**UI**

* **Streamlit** â€“ fast UI, chat interface, file upload, session state.

**RAG Orchestration**

* **LlamaIndex** â€“ clean ingestion/retrieval pipelines, citation support, composable.

**LLM Runtime**

* **Ollama** running **Llamaâ€‘3.1â€‘8Bâ€‘Instruct** (primary)

  * Reason: oneâ€‘binary runtime, easy model mgmt, good DE/EN, CPU/GPU capable.
  * Alternative fallback (only if needed later): llama.cpp directly.

**Embeddings**

* **bgeâ€‘m3** (FlagEmbedding) â€“ multilingual, strong retrieval (DE/EN), CPUâ€‘friendly.

**Reranker**

* **bgeâ€‘rerankerâ€‘base** â€“ lightweight crossâ€‘encoder to refine topâ€‘K â†’ topâ€‘N.

**Vector Store**

* **Qdrant** (single Docker container; persistent volume) â€“ stable, fast, filters/metadata.

**Parsing / OCR**

* **PyMuPDF** for PDFs (fast, reliable)
* **pythonâ€‘docx**, builtâ€‘in readers for TXT/MD
* **Tesseract OCR** (lang packs `eng`, `deu`) â€“ **optional** for scanned PDFs (toggle in UI)

**Storage**

* Local filesystem:

  * `/data/docs` (raw files)
  * `/data/qdrant` (vector store)
  * `/data/cache` (artifacts, chunks)

**Config & Security**

* **pythonâ€‘dotenv** for secrets/config
* **streamlitâ€‘authenticator** (optional) for basic username/password login

**Observability (minimal by default)**

* Structured app logs (JSON), latency counters, retrieval size
* (Optional later) **Langfuse** for traces

**Packaging**

* **Docker Compose** with **3 services**: `app` (Streamlit), `qdrant`, `ollama`

---

# 3) Architecture

**Topology (single server)**

* **Streamlit App (Python)**

  * Hosts UI, ingestion jobs, RAG pipeline orchestration with LlamaIndex
  * Talks to **Ollama** (HTTP) for LLM inference
  * Talks to **Qdrant** (HTTP) for vector search
  * Reads/writes local filesystem for documents & artifacts

**Request/Data Flow**

1. **Upload** â†’ Streamlit saves file â†’ parse (PyMuPDF / docx / txt) â†’ (optional OCR)
2. **Chunk** â†’ token/semantic splitter (300â€“800 tokens) + metadata (doc\_id, page, lang)
3. **Embed** â†’ bgeâ€‘m3 â†’ vectors + metadata â†’ Qdrant collection
4. **Query** â†’ retrieve topâ€‘K (e.g., 30) from Qdrant â†’ **rerank** with bgeâ€‘rerankerâ€‘base â†’ keep 6â€“8
5. **Generate** â†’ contextâ€‘stuffed prompt to Llamaâ€‘3.1â€‘8B via Ollama â†’ stream tokens to UI
6. **Citations** â†’ include (doc, page, snippet) from chunk metadata

**Key Parameters (defaults)**

* Chunk size: \~600 tokens, 60â€“100 tokens overlap
* Retrieval: `top_k=30` â†’ rerank â†’ `top_n=6â€“8`
* Max context fed to LLM: \~4â€“6k tokens (pragmatic for 8B model)
* Language: autoâ€‘detect per user message; mirror user language in responses

**Security & Access**

* Optional login; perâ€‘session document scoping (userâ€‘local collections)
* No multiâ€‘tenant isolation beyond perâ€‘user collections in this first iteration
* File deletion evicts vectors + artifacts for the deleted doc

---

# 4) Data Model (metadata essentials)

**Document**

* `doc_id` (UUID), `filename`, `mime`, `uploaded_by`, `uploaded_at`, `language_guess`, `hash`

**Chunk**

* `chunk_id`, `doc_id`, `text`, `page`, `section_heading?`, `tokens`, `language`
* `embedding` (vector), `created_at`

**Chat Message**

* `session_id`, `role` (user/assistant), `ts`, `question`, `answer`, `citations[]`

---

# 5) Deployment (singleâ€‘server)

**dockerâ€‘compose.yml (sketch)**

* `app`: Streamlit + Python deps; mounts `/data`
* `qdrant`: persistent volume `/data/qdrant`
* `ollama`: persistent model cache; preâ€‘pull `llama3.1:8b`
* Healthchecks for qdrant & ollama before app starts

**Resources**

* CPUâ€‘only OK (slower); 1Ã—GPU (12â€“16GB) recommended for snappy responses
* Disk: plan \~2â€“3Ã— source size for artifacts + vectors

**Config**

* `.env`: model names, chunk sizes, retrieval params, path roots, auth toggle

---

# 6) Nonâ€‘Functional Requirements

* **Performance**: p50 answer latency â‰¤ 4â€“6s on small docs with GPU; CPU: â‰¤ 10â€“15s target
* **Accuracy**: answers always cite sources; if not found, say â€œnot in documentsâ€
* **Stability**: restartâ€‘safe; no data loss (volumes mounted)
* **Simplicity**: â‰¤ 3 containers; minimal knobs exposed in UI
* **i18n**: DE/ENâ€”echo user language; support German diacritics and OCR (`deu`)

---

# 7) Highâ€‘Level Features (Epics)

### Epic A â€” Document Ingestion & Management

**Goal:** Bring documents into the system, parse, index, and manage them.
**Includes:**

* Upload multiple files (PDF/DOCX/TXT/MD)
* Parse + (optional) OCR scans
* Chunking + metadata
* Embedding â†’ Qdrant
* List, delete, reâ€‘index documents
* Show processing progress and failures

### Epic B â€” Chat with Documents

**Goal:** Naturalâ€‘language Q\&A over indexed docs with citations.
**Includes:**

* Chat UI with streaming responses
* Retrieval (topâ€‘K) + reranking (topâ€‘N)
* Contextâ€‘stuffed generation via Ollama
* Citations (doc name + page + snippet)
* Followâ€‘up question suggestions
* Scope chat to â€œall docsâ€ or selected subset

### Epic C â€” Quality & Controls

**Goal:** Keep answers grounded and controllable.
**Includes:**

* â€œShow sourcesâ€ toggle & snippet previews
* â€œOnly answer from docsâ€ mode (refuse unsupported claims)
* Basic answer feedback (ğŸ‘/ğŸ‘) stored locally
* Rate limits to protect server

### Epic D â€” User & Session Management

**Goal:** Lightweight access control and session continuity.
**Includes:**

* Optional login (streamlitâ€‘authenticator)
* Perâ€‘session chat history
* Export chat transcript (TXT/MD)

### Epic E â€” Operations & Observability (Minimal)

**Goal:** Basic operational visibility and safe operation.
**Includes:**

* Structured logging (ingestion, retrieval, generation)
* Simple metrics: request count, latency p50/p95, token usage
* Healthchecks for Ollama/Qdrant
* Admin page: service status, storage usage, reindex all

### Epic F â€” Internationalization (DE/EN)

**Goal:** Smooth bilingual operation.
**Includes:**

* Auto language detection for user queries
* Answer in user language
* OCR language switch (eng/deu)

---

# 8) Out of Scope (initial release)

* Multiâ€‘tenant enterprise RBAC/SSO
* Fineâ€‘grained ACL per document section
* Advanced eval dashboards (e.g., Ragas) and prompt A/B testing
* Nonâ€‘textual extraction (tables/figures layout capture beyond plain text)
* External SaaS vector stores or LLMs

---

# 9) Risks & Mitigations

* **Scanned PDFs degrade quality** â†’ Provide OCR toggle; flag lowâ€‘confidence pages.
* **Large docs slow ingestion** â†’ Background job runner inâ€‘process; progress UI; chunk size caps.
* **CPUâ€‘only latency** â†’ Offer â€œconcise modeâ€ (shorter max tokens) and smaller reranker/LLM knobs.
* **Hallucinations** â†’ â€œOnly from docsâ€ mode; enforce citations; instructive system prompt.

---

# 10) Milestones

1. **MVP (Weeks 1â€“2)**: Ingest (PDF/DOCX/TXT), retrieval + citations, chat UI, delete/reindex, single collection.
2. **Polish (Weeks 3â€“4)**: OCR option, reranker, doc scoping, transcripts, minimal metrics.
3. **Ops (Week 5)**: Admin page, healthchecks, volumes, backup script.
4. **Niceâ€‘toâ€‘have (later)**: Langfuse tracing, batch import, folders/tags, evaluation set.
